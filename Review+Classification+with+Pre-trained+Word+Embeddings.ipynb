{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
  "import numpy as np\n",
  "from sklearn.utils import shuffle\n",
    "from nltk.corpus import stopwords,words,brown\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "from string import punctuation\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Dropout, Conv1D, MaxPooling1D, Activation, Bidirectional\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from keras import layers, regularizers\n",
    "from keras.models import Sequential\n",
    "from keras import models\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('women.csv', encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = data['Review Text'].astype(str)\n",
    "ratings = data['Recommended IND'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts0 = []\n",
    "ratings0 = []\n",
    "texts1 = []\n",
    "ratings1 = []\n",
    "for text, rating in zip(texts, ratings):\n",
    "    if rating == 0:\n",
    "        texts0.append(text)\n",
    "        ratings0.append(rating)\n",
    "    else:\n",
    "        texts1.append(text)\n",
    "        ratings1.append(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts0.extend(texts1[0:len(texts0)])\n",
    "ratings0.extend(ratings1[0:len(ratings0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = texts0\n",
    "Y = ratings0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "Y = to_categorical(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing and removing stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "         return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None # for easy if-statement\n",
    "\n",
    "\n",
    "\n",
    "def filter_stop_words(train_sentences, stop_words):\n",
    "    for i, sentence in enumerate(train_sentences):\n",
    "        tokens = word_tokenize(sentence)\n",
    "        lemmas = []\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        for word, tag in tagged:\n",
    "            if word not in stop_words:\n",
    "                wntag = get_wordnet_pos(tag)\n",
    "                if wntag is None:# not supply tag in case of None\n",
    "                    lemma = lemmatizer.lemmatize(word) \n",
    "                else:\n",
    "                    lemma = lemmatizer.lemmatize(word, pos=wntag)\n",
    "                lemmas.append(lemma)\n",
    "        train_sentences[i] = ' '.join(lemmas)\n",
    "    return train_sentences\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "X = filter_stop_words(X, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "tok = Tokenizer(num_words=2000)\n",
    "tok.fit_on_texts(X)\n",
    "sequences = tok.texts_to_sequences(X)\n",
    "sequences_matrix = pad_sequences(sequences,maxlen=max_len, padding = 'post')\n",
    "train_data = np.array(sequences_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(train_data,Y,test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating matrix of pre-trained word embeddings from GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(r\"C:\\Users\\Abhishek\\Downloads\\glove.6B\\glove.6B.100d.txt\", encoding = 'utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tok.word_index) + 1, 100))\n",
    "for word, i in tok.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(tok.word_index) + 1,\n",
    "                            100,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=50,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5673 samples, validate on 1419 samples\n",
      "Epoch 1/25\n",
      "5673/5673 [==============================] - 42s 7ms/step - loss: 1.4311 - categorical_accuracy: 0.5660 - val_loss: 1.0843 - val_categorical_accuracy: 0.6582\n",
      "Epoch 2/25\n",
      "5673/5673 [==============================] - 10s 2ms/step - loss: 0.9160 - categorical_accuracy: 0.6559 - val_loss: 0.6988 - val_categorical_accuracy: 0.7414\n",
      "Epoch 3/25\n",
      "5673/5673 [==============================] - 15s 3ms/step - loss: 0.6601 - categorical_accuracy: 0.7271 - val_loss: 0.5604 - val_categorical_accuracy: 0.7689\n",
      "Epoch 4/25\n",
      "5673/5673 [==============================] - 10s 2ms/step - loss: 0.5446 - categorical_accuracy: 0.7592 - val_loss: 0.4782 - val_categorical_accuracy: 0.7893\n",
      "Epoch 5/25\n",
      "5673/5673 [==============================] - 10s 2ms/step - loss: 0.4915 - categorical_accuracy: 0.7846 - val_loss: 0.4667 - val_categorical_accuracy: 0.7992\n",
      "Epoch 6/25\n",
      "5673/5673 [==============================] - 10s 2ms/step - loss: 0.4632 - categorical_accuracy: 0.7964 - val_loss: 0.4257 - val_categorical_accuracy: 0.8118\n",
      "Epoch 7/25\n",
      "5673/5673 [==============================] - 10s 2ms/step - loss: 0.4374 - categorical_accuracy: 0.8010 - val_loss: 0.4403 - val_categorical_accuracy: 0.8097\n",
      "Epoch 8/25\n",
      "5673/5673 [==============================] - 12s 2ms/step - loss: 0.4226 - categorical_accuracy: 0.8100 - val_loss: 0.3852 - val_categorical_accuracy: 0.8288\n",
      "Epoch 9/25\n",
      "5673/5673 [==============================] - 10s 2ms/step - loss: 0.4092 - categorical_accuracy: 0.8213 - val_loss: 0.4062 - val_categorical_accuracy: 0.8224\n",
      "Epoch 10/25\n",
      "5673/5673 [==============================] - 10s 2ms/step - loss: 0.3825 - categorical_accuracy: 0.8325 - val_loss: 0.4001 - val_categorical_accuracy: 0.8245\n",
      "Epoch 11/25\n",
      "5673/5673 [==============================] - 10s 2ms/step - loss: 0.3808 - categorical_accuracy: 0.8371 - val_loss: 0.3745 - val_categorical_accuracy: 0.8372\n",
      "Epoch 12/25\n",
      "5673/5673 [==============================] - 10s 2ms/step - loss: 0.3663 - categorical_accuracy: 0.8394 - val_loss: 0.3659 - val_categorical_accuracy: 0.8351\n",
      "Epoch 13/25\n",
      "5673/5673 [==============================] - 10s 2ms/step - loss: 0.3605 - categorical_accuracy: 0.8412 - val_loss: 0.3640 - val_categorical_accuracy: 0.8428\n",
      "Epoch 14/25\n",
      "5673/5673 [==============================] - 10s 2ms/step - loss: 0.3507 - categorical_accuracy: 0.8525 - val_loss: 0.3657 - val_categorical_accuracy: 0.8393\n",
      "Epoch 15/25\n",
      "5673/5673 [==============================] - 13s 2ms/step - loss: 0.3464 - categorical_accuracy: 0.8548 - val_loss: 0.3703 - val_categorical_accuracy: 0.8457\n",
      "Epoch 16/25\n",
      "5673/5673 [==============================] - 14s 3ms/step - loss: 0.3386 - categorical_accuracy: 0.8588 - val_loss: 0.3711 - val_categorical_accuracy: 0.8414\n",
      "Epoch 17/25\n",
      "5673/5673 [==============================] - 14s 2ms/step - loss: 0.3289 - categorical_accuracy: 0.8604 - val_loss: 0.3649 - val_categorical_accuracy: 0.8386\n",
      "Epoch 18/25\n",
      "5673/5673 [==============================] - 14s 2ms/step - loss: 0.3197 - categorical_accuracy: 0.8623 - val_loss: 0.3717 - val_categorical_accuracy: 0.8414\n",
      "Epoch 19/25\n",
      "5673/5673 [==============================] - 13s 2ms/step - loss: 0.3145 - categorical_accuracy: 0.8655 - val_loss: 0.3688 - val_categorical_accuracy: 0.8471\n",
      "Epoch 20/25\n",
      "5673/5673 [==============================] - 13s 2ms/step - loss: 0.3042 - categorical_accuracy: 0.8704 - val_loss: 0.3602 - val_categorical_accuracy: 0.8478\n",
      "Epoch 21/25\n",
      "5673/5673 [==============================] - 14s 2ms/step - loss: 0.2980 - categorical_accuracy: 0.8794 - val_loss: 0.3914 - val_categorical_accuracy: 0.8428\n",
      "Epoch 22/25\n",
      "5673/5673 [==============================] - 13s 2ms/step - loss: 0.3009 - categorical_accuracy: 0.8741 - val_loss: 0.3818 - val_categorical_accuracy: 0.8414\n",
      "Epoch 23/25\n",
      "5673/5673 [==============================] - 14s 2ms/step - loss: 0.2948 - categorical_accuracy: 0.8770 - val_loss: 0.3731 - val_categorical_accuracy: 0.8513\n",
      "Epoch 24/25\n",
      "5673/5673 [==============================] - 13s 2ms/step - loss: 0.2994 - categorical_accuracy: 0.8798 - val_loss: 0.3666 - val_categorical_accuracy: 0.8428\n",
      "Epoch 25/25\n",
      "5673/5673 [==============================] - 14s 2ms/step - loss: 0.2804 - categorical_accuracy: 0.8800 - val_loss: 0.3643 - val_categorical_accuracy: 0.8485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a274954f60>"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(32, dropout = 0.2, kernel_regularizer=regularizers.l2(0.01))))\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "from keras.optimizers import SGD\n",
    "opt = SGD(lr=0.001)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = 'adam', metrics = ['categorical_accuracy'])\n",
    "model.fit(X_train,Y_train,batch_size=128,epochs=25,validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[573,  76],\n",
       "       [100, 503]], dtype=int64)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "matrix = metrics.confusion_matrix(Y_test.argmax(axis=1), Y_pred.argmax(axis=1))\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_score = metrics.f1_score(Y_test.argmax(axis=1), Y_pred.argmax(axis=1), average = 'weighted')\n",
    "accuracy = metrics.accuracy_score(Y_test.argmax(axis=1), Y_pred.argmax(axis=1))\n",
    "precision = metrics.precision_score(Y_test.argmax(axis=1), Y_pred.argmax(axis=1), average = 'weighted')\n",
    "recall = metrics.recall_score(Y_test.argmax(axis=1), Y_pred.argmax(axis=1), average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.86, the precision is 0.86, the recall is 0.86 and the f1 score is 0.86.\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy is ' + str((\"%.2f\" % accuracy)) + ', the precision is ' + str((\"%.2f\" % precision)) + ', the recall is ' + str((\"%.2f\" % precision)) + ' and the f1 score is ' + str((\"%.2f\" % f1_score)) + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
